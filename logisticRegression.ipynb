{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiment on top 5 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['data/elections', 'data/politics', 'data/white_house', 'data/immigration', 'data/healthcare']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "runid = datetime.now().strftime(\"%d-%m-%y%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_saveplace(runid):\n",
    "    route = os.path.join('.', 'runs', f'lr-run-{runid}')\n",
    "    os.makedirs(route, exist_ok=True)\n",
    "    return route\n",
    "\n",
    "saveplace = init_saveplace(runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(url):\n",
    "    train_data = pd.read_csv(f'{url}/train/{url.split(\"/\")[1]}_train.csv')\n",
    "    train_data = train_data[(train_data['bias'] == 0) | (train_data['bias'] == 2)]\n",
    "    train_data['content'] = train_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    test_data = pd.read_csv(f'{url}/test/{url.split(\"/\")[1]}_test.csv')\n",
    "    test_data = test_data[(test_data['bias'] == 0) | (test_data['bias'] == 2)]\n",
    "    test_data['content'] = test_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    tfid = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    x_train = train_data['content'].values\n",
    "    x_train = tfid.fit_transform(x_train)\n",
    "    y_train = train_data['bias'].values\n",
    "    \n",
    "    x_test = test_data['content'].values\n",
    "    x_test = tfid.transform(x_test)\n",
    "    y_test = test_data['bias'].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_data(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=2)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=2)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=2)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return acc, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiment(url):\n",
    "    x_train, y_train, x_test, y_test = preprocess_data(url)\n",
    "    model = train_model(x_train, y_train)\n",
    "    return predict_on_test_data(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elections Accuracy: 0.65846\n",
      "elections Precision: 0.66995\n",
      "elections Recall: 0.41531\n",
      "elections F1: 0.51276\n",
      "\n",
      "\n",
      "politics Accuracy: 0.66069\n",
      "politics Precision: 0.63068\n",
      "politics Recall: 0.86183\n",
      "politics F1: 0.72835\n",
      "\n",
      "\n",
      "white_house Accuracy: 0.64510\n",
      "white_house Precision: 0.64297\n",
      "white_house Recall: 0.73111\n",
      "white_house F1: 0.68421\n",
      "\n",
      "\n",
      "immigration Accuracy: 0.66230\n",
      "immigration Precision: 0.64746\n",
      "immigration Recall: 0.90076\n",
      "immigration F1: 0.75339\n",
      "\n",
      "\n",
      "healthcare Accuracy: 0.62237\n",
      "healthcare Precision: 0.59067\n",
      "healthcare Recall: 0.94802\n",
      "healthcare F1: 0.72785\n",
      "\n",
      "\n",
      "Average Accuracy: 0.64978\n",
      "Average Precision: 0.63635\n",
      "Average Recall: 0.77141\n",
      "Average F1: 0.68131\n"
     ]
    }
   ],
   "source": [
    "scores = pd.DataFrame(None, urls, ['F1 Macro', 'F1 Micro', 'Precision', 'Recall'])\n",
    "cms = pd.DataFrame(None, urls, ['True Left', 'False Left', 'True Right', \"False Right\"])\n",
    "\n",
    "for url in urls:\n",
    "    topic = url.split(\"/\")[1]\n",
    "    \n",
    "    accuracy, precision, recall, f1, cm = runExperiment(url)\n",
    "\n",
    "    scores['F1 Macro'][url] = f1\n",
    "    scores['F1 Micro'][url] = accuracy\n",
    "    scores['Precision'][url] = precision\n",
    "    scores['Recall'][url] = recall\n",
    "    \n",
    "    cms['True Left'][url] = cm[0,0]\n",
    "    cms['False Left'][url] = cm[1,0]\n",
    "    cms['True Right'][url] = cm[1,1]\n",
    "    cms['False Right'][url] = cm[0,1]\n",
    "    \n",
    "    print(f'{topic} Accuracy: {accuracy:.5f}')\n",
    "    print(f'{topic} Precision: {precision:.5f}')\n",
    "    print(f'{topic} Recall: {recall:.5f}')\n",
    "    print(f'{topic} F1: {f1:.5f}\\n\\n')\n",
    "\n",
    "averages = scores.mean(axis=0)\n",
    "\n",
    "print(f'Average Accuracy: {averages[\"F1 Micro\"]:.5f}')\n",
    "print(f'Average Precision: {averages[\"Precision\"]:.5f}')\n",
    "print(f'Average Recall: {averages[\"Recall\"]:.5f}')\n",
    "print(f'Average F1: {averages[\"F1 Macro\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info = pd.concat([scores, cms], axis=1)\n",
    "combined_info.to_csv(os.path.join(saveplace, f'lr_nodr_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.681313</td>\n",
       "      <td>0.649783</td>\n",
       "      <td>0.636345</td>\n",
       "      <td>0.771407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F1 Macro  F1 Micro  Precision    Recall\n",
       "Logistic Regression  0.681313  0.649783   0.636345  0.771407"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['F1 Macro', 'F1 Micro', 'Precision', 'Recall']\n",
    "data = [averages[cols]]\n",
    "data_metrics = pd.DataFrame(data, columns=cols, index=['Logistic Regression'])\n",
    "data_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiment on top 5 categories using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_PCA(url):\n",
    "    train_data = pd.read_csv(f'{url}/train/{url.split(\"/\")[1]}_train.csv')\n",
    "    train_data = train_data[(train_data['bias'] == 0) | (train_data['bias'] == 2)]\n",
    "    train_data['content'] = train_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    test_data = pd.read_csv(f'{url}/test/{url.split(\"/\")[1]}_test.csv')\n",
    "    test_data = test_data[(test_data['bias'] == 0) | (test_data['bias'] == 2)]\n",
    "    test_data['content'] = test_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    tfid = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    x_train = train_data['content'].values\n",
    "    x_train = tfid.fit_transform(x_train)\n",
    "    y_train = train_data['bias'].values\n",
    "    \n",
    "    x_test = test_data['content'].values\n",
    "    x_test = tfid.transform(x_test)\n",
    "    y_test = test_data['bias'].values\n",
    "    \n",
    "    \"\"\" \n",
    "    We utilized the default version of PCA from sklearn instead of using class-based code. \n",
    "    This allows us to use the same formatting as a slot-in replacement for the other dimensionality \n",
    "    reduction techniques we evaluated or tested in this project, like Sparse PCA and Truncated SVD. \n",
    "    Furthermore, due to the scale and sparsity of our text data, utilizing the sklearn PCA, which is \n",
    "    optimized for speed, made our computations more efficient. \n",
    "    \"\"\"\n",
    "    \n",
    "    pca = PCA(n_components=0.9)\n",
    "    transformed_x_train = pca.fit_transform(x_train.toarray())\n",
    "    transformed_x_test = pca.transform(x_test.toarray())\n",
    "    \n",
    "    return transformed_x_train, y_train, transformed_x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiment_with_pca(url):\n",
    "    x_train, y_train, x_test, y_test = preprocess_data_with_PCA(url)\n",
    "    model = train_model(x_train, y_train)\n",
    "    return predict_on_test_data(model, x_test, y_test), x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components of X_test: 611\n",
      "elections Accuracy: 0.6564\n",
      "elections Precision: 0.6652\n",
      "elections Recall: 0.4146\n",
      "elections F1: 0.5109\n",
      "\n",
      "\n",
      "Components of X_test: 314\n",
      "politics Accuracy: 0.6539\n",
      "politics Precision: 0.6265\n",
      "politics Recall: 0.8525\n",
      "politics F1: 0.7222\n",
      "\n",
      "\n",
      "Components of X_test: 214\n",
      "white_house Accuracy: 0.6414\n",
      "white_house Precision: 0.6425\n",
      "white_house Recall: 0.7170\n",
      "white_house F1: 0.6777\n",
      "\n",
      "\n",
      "Components of X_test: 179\n",
      "immigration Accuracy: 0.6612\n",
      "immigration Precision: 0.6474\n",
      "immigration Recall: 0.8969\n",
      "immigration F1: 0.7520\n",
      "\n",
      "\n",
      "Components of X_test: 173\n",
      "healthcare Accuracy: 0.6202\n",
      "healthcare Precision: 0.5894\n",
      "healthcare Recall: 0.9459\n",
      "healthcare F1: 0.7263\n",
      "\n",
      "\n",
      "Average Accuracy: 0.64661\n",
      "Average Precision: 0.63420\n",
      "Average Recall: 0.76541\n",
      "Average F1: 0.67781\n"
     ]
    }
   ],
   "source": [
    "scores = pd.DataFrame(None, urls, ['F1 Macro', 'F1 Micro', 'Precision', 'Recall', 'Components'])\n",
    "cms = pd.DataFrame(None, urls, ['True Left', 'False Left', 'True Right', \"False Right\"])\n",
    "\n",
    "for url in urls:\n",
    "    topic = url.split(\"/\")[1]\n",
    "    \n",
    "    experiment_results, x_test = runExperiment_with_pca(url)\n",
    "    print(f\"Components of X_test: {x_test.shape[1]}\")\n",
    "    scores['Components'][url] = x_test.shape[1]\n",
    "\n",
    "    accuracy, precision, recall, f1, cm = experiment_results\n",
    "    \n",
    "    scores['F1 Macro'][url] = f1\n",
    "    scores['F1 Micro'][url] = accuracy\n",
    "    scores['Precision'][url] = precision\n",
    "    scores['Recall'][url] = recall\n",
    "    \n",
    "    cms['True Left'][url] = cm[0,0]\n",
    "    cms['False Left'][url] = cm[1,0]\n",
    "    cms['True Right'][url] = cm[1,1]\n",
    "    cms['False Right'][url] = cm[0,1]\n",
    "    \n",
    "    print(f'{topic} Accuracy: {accuracy:.4f}')\n",
    "    print(f'{topic} Precision: {precision:.4f}')\n",
    "    print(f'{topic} Recall: {recall:.4f}')\n",
    "    print(f'{topic} F1: {f1:.4f}\\n\\n')\n",
    "    \n",
    "averages = scores.mean(axis=0)\n",
    "\n",
    "print(f'Average Accuracy: {averages[\"F1 Micro\"]:.5f}')\n",
    "print(f'Average Precision: {averages[\"Precision\"]:.5f}')\n",
    "print(f'Average Recall: {averages[\"Recall\"]:.5f}')\n",
    "print(f'Average F1: {averages[\"F1 Macro\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info = pd.concat([scores, cms], axis=1)\n",
    "combined_info.to_csv(os.path.join(saveplace, f'lr_pca_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.681313</td>\n",
       "      <td>0.649783</td>\n",
       "      <td>0.636345</td>\n",
       "      <td>0.771407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression with PCA</th>\n",
       "      <td>0.677814</td>\n",
       "      <td>0.646612</td>\n",
       "      <td>0.634201</td>\n",
       "      <td>0.765407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              F1 Macro  F1 Micro  Precision    Recall\n",
       "Logistic Regression           0.681313  0.649783   0.636345  0.771407\n",
       "Logistic Regression with PCA  0.677814  0.646612   0.634201  0.765407"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['F1 Macro', 'F1 Micro', 'Precision', 'Recall']\n",
    "data = [averages[cols]]\n",
    "new_data_metrics = pd.DataFrame(data, columns=cols, index=['Logistic Regression with PCA'])\n",
    "\n",
    "result = pd.concat([data_metrics, new_data_metrics])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiment on top 5 categories using TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_SPCA(url):\n",
    "    topic = url.split(\"/\")[1]\n",
    "    train_data = pd.read_csv(f'{url}/train/{topic}_train.csv')\n",
    "    train_data = train_data[(train_data['bias'] == 0) | (train_data['bias'] == 2)]\n",
    "    train_data['content'] = train_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    test_data = pd.read_csv(f'{url}/test/{topic}_test.csv')\n",
    "    test_data = test_data[(test_data['bias'] == 0) | (test_data['bias'] == 2)]\n",
    "    test_data['content'] = test_data['content'].apply(lambda x: x.lower())\n",
    "    \n",
    "    tfid = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    x_train = train_data['content'].values\n",
    "    x_train = tfid.fit_transform(x_train)\n",
    "    y_train = train_data['bias'].values\n",
    "    \n",
    "    x_test = test_data['content'].values\n",
    "    x_test = tfid.transform(x_test)\n",
    "    y_test = test_data['bias'].values\n",
    "    \n",
    "    # spca = SparsePCA(n_components=400)\n",
    "    # transformed_x_train = spca.fit_transform(x_train.toarray())\n",
    "    # transformed_x_test = spca.transform(x_test.toarray())\n",
    "    \n",
    "    num_components = {'elections': 600,  'politics': 311, 'white_house': 211, 'immigration': 176, 'healthcare': 170}\n",
    "\n",
    "    svd = TruncatedSVD(n_components=num_components[topic])\n",
    "    transformed_x_train = svd.fit_transform(x_train)\n",
    "    transformed_x_test = svd.transform(x_test)\n",
    "    \n",
    "    # explained_variance = (svd.singular_values_ ** 2) / (np.sum(svd.singular_values_ ** 2))\n",
    "    # cumulative_variance = np.cumsum(explained_variance)\n",
    "    # n_componenet_needed = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "    # topic = url.split(\"/\")[1]\n",
    "    # print(f\"Number of components needed to capture 90% variance for topic {topic}: {n_componenet_needed}\")\n",
    "    \n",
    "    return transformed_x_train, y_train, transformed_x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiment_with_spca(url):\n",
    "    x_train, y_train, x_test, y_test = preprocess_data_with_SPCA(url)\n",
    "    model = train_model(x_train, y_train)\n",
    "    return predict_on_test_data(model, x_test, y_test), x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components of X_test: 600\n",
      "elections Accuracy: 0.6579\n",
      "elections Precision: 0.6692\n",
      "elections Recall: 0.4140\n",
      "elections F1: 0.5115\n",
      "\n",
      "\n",
      "Components of X_test: 311\n",
      "politics Accuracy: 0.6545\n",
      "politics Precision: 0.6264\n",
      "politics Recall: 0.8560\n",
      "politics F1: 0.7234\n",
      "\n",
      "\n",
      "Components of X_test: 211\n",
      "white_house Accuracy: 0.6386\n",
      "white_house Precision: 0.6378\n",
      "white_house Recall: 0.7241\n",
      "white_house F1: 0.6782\n",
      "\n",
      "\n",
      "Components of X_test: 176\n",
      "immigration Accuracy: 0.6612\n",
      "immigration Precision: 0.6462\n",
      "immigration Recall: 0.9027\n",
      "immigration F1: 0.7532\n",
      "\n",
      "\n",
      "Components of X_test: 170\n",
      "healthcare Accuracy: 0.6235\n",
      "healthcare Precision: 0.5914\n",
      "healthcare Recall: 0.9480\n",
      "healthcare F1: 0.7284\n",
      "\n",
      "\n",
      "Average Accuracy: 0.64714\n",
      "Average Precision: 0.63420\n",
      "Average Recall: 0.76894\n",
      "Average F1: 0.67894\n"
     ]
    }
   ],
   "source": [
    "scores = pd.DataFrame(None, urls, ['F1 Macro', 'F1 Micro', 'Precision', 'Recall', 'Components'])\n",
    "cms = pd.DataFrame(None, urls, ['True Left', 'False Left', 'True Right', \"False Right\"])\n",
    "\n",
    "for url in urls:\n",
    "    topic = url.split(\"/\")[1]\n",
    "    \n",
    "    experiment_results, x_test = runExperiment_with_spca(url)\n",
    "    print(f\"Components of X_test: {x_test.shape[1]}\")\n",
    "    scores['Components'][url] = x_test.shape[1]\n",
    "\n",
    "    accuracy, precision, recall, f1, cm = experiment_results\n",
    "    \n",
    "    scores['F1 Macro'][url] = f1\n",
    "    scores['F1 Micro'][url] = accuracy\n",
    "    scores['Precision'][url] = precision\n",
    "    scores['Recall'][url] = recall\n",
    "    \n",
    "    cms['True Left'][url] = cm[0,0]\n",
    "    cms['False Left'][url] = cm[1,0]\n",
    "    cms['True Right'][url] = cm[1,1]\n",
    "    cms['False Right'][url] = cm[0,1]\n",
    "    \n",
    "    print(f'{topic} Accuracy: {accuracy:.4f}')\n",
    "    print(f'{topic} Precision: {precision:.4f}')\n",
    "    print(f'{topic} Recall: {recall:.4f}')\n",
    "    print(f'{topic} F1: {f1:.4f}\\n\\n')\n",
    "    \n",
    "averages = scores.mean(axis=0)\n",
    "\n",
    "print(f'Average Accuracy: {averages[\"F1 Micro\"]:.5f}')\n",
    "print(f'Average Precision: {averages[\"Precision\"]:.5f}')\n",
    "print(f'Average Recall: {averages[\"Recall\"]:.5f}')\n",
    "print(f'Average F1: {averages[\"F1 Macro\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info = pd.concat([scores, cms], axis=1)\n",
    "combined_info.to_csv(os.path.join(saveplace, f'lr_tsvd_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.681313</td>\n",
       "      <td>0.649783</td>\n",
       "      <td>0.636345</td>\n",
       "      <td>0.771407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression with PCA</th>\n",
       "      <td>0.677814</td>\n",
       "      <td>0.646612</td>\n",
       "      <td>0.634201</td>\n",
       "      <td>0.765407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression with TruncatedSVD</th>\n",
       "      <td>0.678945</td>\n",
       "      <td>0.647139</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.768941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       F1 Macro  F1 Micro  Precision    Recall\n",
       "Logistic Regression                    0.681313  0.649783   0.636345  0.771407\n",
       "Logistic Regression with PCA           0.677814  0.646612   0.634201  0.765407\n",
       "Logistic Regression with TruncatedSVD  0.678945  0.647139   0.634200  0.768941"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['F1 Macro', 'F1 Micro', 'Precision', 'Recall']\n",
    "data = [averages[cols]]\n",
    "new_data_metrics = pd.DataFrame(data, columns=cols, index=['Logistic Regression with TruncatedSVD'])\n",
    "\n",
    "result = pd.concat([result, new_data_metrics])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(os.path.join(saveplace, f'summarized_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI_4022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
