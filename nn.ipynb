{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stop_words as sw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import average_precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['data/elections', 'data/politics', 'data/white_house', 'data/immigration', 'data/healthcare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(url):\n",
    "    train_data = pd.read_csv(f'{url}/train/{url.split(\"/\")[1]}_train.csv')\n",
    "    train_data = train_data[(train_data['bias'] == 0) | (train_data['bias'] == 2)].replace({'bias': {2: 1}})\n",
    "    train_data['stop_content'] = train_data['content'].apply(lambda x: ' '.join(sw.stop_words(x)))\n",
    "    \n",
    "    test_data = pd.read_csv(f'{url}/test/{url.split(\"/\")[1]}_test.csv')\n",
    "    test_data = test_data[(test_data['bias'] == 0) | (test_data['bias'] == 2)].replace({'bias': {2: 1}})\n",
    "    test_data['stop_content'] = test_data['content'].apply(lambda x: ' '.join(sw.stop_words(x)))\n",
    "    \n",
    "    tfid = TfidfVectorizer()\n",
    "    \n",
    "    x_train = tfid.fit_transform(train_data['stop_content'])\n",
    "    y_train = train_data['bias'].values\n",
    "    \n",
    "    x_test = tfid.transform(test_data['stop_content'])\n",
    "    y_test = test_data['bias'].values\n",
    "\n",
    "    # We utilized the default version of PCA from sklearn instead of using class-based code.\n",
    "    # This allows us to use the same formatting as a slot-in replacement for the other\n",
    "    # dimensionality reduction techniques we evaluated or tested in this project, like\n",
    "    # Sparse PCA and Truncated SVD. Furthermore, due to the scale and sparsity of our text data,\n",
    "    # utilizing the sklearn PCA, which is optimized for speed, made our computations more efficient. \n",
    "    # pca = PCA(n_components=0.9)\n",
    "    # transformed_x_train = pca.fit_transform(x_train.toarray())\n",
    "    # transformed_x_test = pca.transform(x_test.toarray())\n",
    "    \n",
    "    num_components = {'elections': 600,  'politics': 311, 'white_house': 211, 'immigration': 176, 'healthcare': 170}\n",
    "\n",
    "    svd = TruncatedSVD(n_components=num_components[topic])\n",
    "    transformed_x_train = svd.fit_transform(x_train)\n",
    "    transformed_x_test = svd.transform(x_test)\n",
    "\n",
    "    return transformed_x_train, y_train, transformed_x_test, y_test, train_data, test_data\n",
    "\n",
    "    # return x_train, y_train, x_test, y_test, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepText(nn.Module):\n",
    "    def __init__(self, tfidf_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(tfidf_size, 200)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(200, 100)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(100, 50)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val, n_epochs=30, batch_size=10):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    batch_start = torch.arange(0, X_train.shape[0], batch_size)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(url, need_dense=True):\n",
    "    def xy_torchly(x, y):\n",
    "        x_torch = torch.tensor(x.todense() if need_dense else x, dtype=torch.float32)\n",
    "        y_torch = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "        return x_torch, y_torch\n",
    "    x_train, y_train, x_test, y_test, _, test_data = preprocess_data(url)\n",
    "    x_train, y_train = xy_torchly(x_train, y_train)\n",
    "    x_test, y_test = xy_torchly(x_test, y_test)\n",
    "    return x_train, y_train, x_test, y_test, test_data\n",
    "\n",
    "def init_saveplace():\n",
    "    current = datetime.now().strftime(\"%d-%m-%y%H:%M:%S\")\n",
    "    route = os.path.join('.', 'runs', f'nn-run-{current}')\n",
    "    os.makedirs(route, exist_ok=True)\n",
    "    return route\n",
    "\n",
    "def train_and_save(x_train, y_train, x_test, y_test, saveplace, topic):\n",
    "    # Generate model\n",
    "    vec_size = x_train.shape[1]\n",
    "    model = DeepText(vec_size)\n",
    "\n",
    "    _ = model_train(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(saveplace, f'model_{topic}.pt'))\n",
    "    return model\n",
    "\n",
    "def test_and_save(model, x_test, test_data, saveplace, topic):\n",
    "    test_data['prediction'] = np.array([model(x).detach().numpy() for x in x_test])\n",
    "    test_data['pred_bias'] = test_data['prediction'].round().astype(int)\n",
    "\n",
    "    test_data.to_csv(os.path.join(saveplace, f'test_data_{topic}.csv'), index=False)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(url, data):\n",
    "    confusion_matrix = pd.DataFrame(0, [url], ['True Left', 'False Left', 'True Right', 'False Right'])\n",
    "    for prediction, actual, tag in [[0,0,'True Left'], [0,1,'False Left'], [1,0,'False Right'], [1,1,'True Right']]:\n",
    "        confusion_matrix[tag] = \\\n",
    "            data[(data['pred_bias'] == prediction) & (data['bias'] == actual)].count()['pred_bias']\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: data/elections (about elections)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 83/83 [00:00<00:00, 95.25batch/s, acc=1, loss=0.665]   \n",
      "Epoch 1: 100%|██████████| 83/83 [00:00<00:00, 108.20batch/s, acc=1, loss=0.652]  \n",
      "Epoch 2: 100%|██████████| 83/83 [00:01<00:00, 42.76batch/s, acc=1, loss=0.63]    \n",
      "Epoch 3: 100%|██████████| 83/83 [00:02<00:00, 31.44batch/s, acc=1, loss=0.598]   \n",
      "Epoch 4: 100%|██████████| 83/83 [00:01<00:00, 58.70batch/s, acc=1, loss=0.556]   \n",
      "Epoch 5: 100%|██████████| 83/83 [00:01<00:00, 78.74batch/s, acc=1, loss=0.475]   \n",
      "Epoch 6: 100%|██████████| 83/83 [00:01<00:00, 58.57batch/s, acc=1, loss=0.314]   \n",
      "Epoch 7: 100%|██████████| 83/83 [00:01<00:00, 63.57batch/s, acc=1, loss=0.131]   \n",
      "Epoch 8: 100%|██████████| 83/83 [00:01<00:00, 77.28batch/s, acc=1, loss=0.0394]  \n",
      "Epoch 9: 100%|██████████| 83/83 [00:01<00:00, 72.05batch/s, acc=1, loss=0.0128] \n",
      "Epoch 10: 100%|██████████| 83/83 [00:01<00:00, 75.73batch/s, acc=1, loss=0.00521] \n",
      "Epoch 11: 100%|██████████| 83/83 [00:01<00:00, 48.81batch/s, acc=1, loss=0.0026]  \n",
      "Epoch 12: 100%|██████████| 83/83 [00:01<00:00, 78.27batch/s, acc=1, loss=0.00149] \n",
      "Epoch 13: 100%|██████████| 83/83 [00:01<00:00, 59.67batch/s, acc=1, loss=0.000935] \n",
      "Epoch 14: 100%|██████████| 83/83 [00:01<00:00, 68.29batch/s, acc=1, loss=0.000613]\n",
      "Epoch 15: 100%|██████████| 83/83 [00:01<00:00, 72.62batch/s, acc=1, loss=0.00042] \n",
      "Epoch 16: 100%|██████████| 83/83 [00:01<00:00, 61.68batch/s, acc=1, loss=0.000299] \n",
      "Epoch 17: 100%|██████████| 83/83 [00:01<00:00, 62.96batch/s, acc=1, loss=0.000221]\n",
      "Epoch 18: 100%|██████████| 83/83 [00:02<00:00, 28.27batch/s, acc=1, loss=0.000166]\n",
      "Epoch 19: 100%|██████████| 83/83 [00:01<00:00, 42.24batch/s, acc=1, loss=0.000128]\n",
      "Epoch 20: 100%|██████████| 83/83 [00:01<00:00, 77.36batch/s, acc=1, loss=0.000101]\n",
      "Epoch 21: 100%|██████████| 83/83 [00:01<00:00, 60.82batch/s, acc=1, loss=8.06e-5] \n",
      "Epoch 22: 100%|██████████| 83/83 [00:02<00:00, 29.38batch/s, acc=1, loss=6.56e-5] \n",
      "Epoch 23: 100%|██████████| 83/83 [00:02<00:00, 37.20batch/s, acc=1, loss=5.4e-5]  \n",
      "Epoch 24: 100%|██████████| 83/83 [00:01<00:00, 60.41batch/s, acc=1, loss=4.48e-5] \n",
      "Epoch 25: 100%|██████████| 83/83 [00:02<00:00, 41.21batch/s, acc=1, loss=3.76e-5] \n",
      "Epoch 26: 100%|██████████| 83/83 [00:02<00:00, 33.87batch/s, acc=1, loss=3.19e-5] \n",
      "Epoch 27: 100%|██████████| 83/83 [00:02<00:00, 38.90batch/s, acc=1, loss=2.72e-5]  \n",
      "Epoch 28: 100%|██████████| 83/83 [00:01<00:00, 50.48batch/s, acc=1, loss=2.34e-5] \n",
      "Epoch 29: 100%|██████████| 83/83 [00:01<00:00, 56.09batch/s, acc=1, loss=2.02e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/elections\n",
      "                True Left  False Left  True Right  False Right\n",
      "data/elections       1462         582         894          473\n",
      "\n",
      "Dataset: data/politics (about politics)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 41/41 [00:00<00:00, 64.01batch/s, acc=0.333, loss=0.694]\n",
      "Epoch 1: 100%|██████████| 41/41 [00:01<00:00, 38.60batch/s, acc=0.667, loss=0.692]\n",
      "Epoch 2: 100%|██████████| 41/41 [00:00<00:00, 60.13batch/s, acc=0.667, loss=0.689]\n",
      "Epoch 3: 100%|██████████| 41/41 [00:00<00:00, 46.17batch/s, acc=0.667, loss=0.686]\n",
      "Epoch 4: 100%|██████████| 41/41 [00:01<00:00, 22.55batch/s, acc=0.667, loss=0.682]\n",
      "Epoch 5: 100%|██████████| 41/41 [00:01<00:00, 34.42batch/s, acc=0.667, loss=0.677]\n",
      "Epoch 6: 100%|██████████| 41/41 [00:01<00:00, 32.89batch/s, acc=0.667, loss=0.669]\n",
      "Epoch 7: 100%|██████████| 41/41 [00:01<00:00, 30.73batch/s, acc=0.667, loss=0.658]\n",
      "Epoch 8: 100%|██████████| 41/41 [00:01<00:00, 40.12batch/s, acc=1, loss=0.64]    \n",
      "Epoch 9: 100%|██████████| 41/41 [00:01<00:00, 40.67batch/s, acc=1, loss=0.61]   \n",
      "Epoch 10: 100%|██████████| 41/41 [00:01<00:00, 37.17batch/s, acc=1, loss=0.565]  \n",
      "Epoch 11: 100%|██████████| 41/41 [00:00<00:00, 44.72batch/s, acc=1, loss=0.504]  \n",
      "Epoch 12: 100%|██████████| 41/41 [00:00<00:00, 48.83batch/s, acc=1, loss=0.426]  \n",
      "Epoch 13: 100%|██████████| 41/41 [00:00<00:00, 42.22batch/s, acc=1, loss=0.334]   \n",
      "Epoch 14: 100%|██████████| 41/41 [00:01<00:00, 34.14batch/s, acc=1, loss=0.243]  \n",
      "Epoch 15: 100%|██████████| 41/41 [00:01<00:00, 38.46batch/s, acc=1, loss=0.167]  \n",
      "Epoch 16: 100%|██████████| 41/41 [00:01<00:00, 24.47batch/s, acc=1, loss=0.113]  \n",
      "Epoch 17: 100%|██████████| 41/41 [00:02<00:00, 18.10batch/s, acc=1, loss=0.0762] \n",
      "Epoch 18: 100%|██████████| 41/41 [00:02<00:00, 14.92batch/s, acc=1, loss=0.0533] \n",
      "Epoch 19: 100%|██████████| 41/41 [00:01<00:00, 29.41batch/s, acc=1, loss=0.0388]  \n",
      "Epoch 20: 100%|██████████| 41/41 [00:01<00:00, 24.18batch/s, acc=1, loss=0.0294] \n",
      "Epoch 21: 100%|██████████| 41/41 [00:02<00:00, 17.96batch/s, acc=1, loss=0.023]  \n",
      "Epoch 22: 100%|██████████| 41/41 [00:01<00:00, 37.66batch/s, acc=1, loss=0.0183] \n",
      "Epoch 23: 100%|██████████| 41/41 [00:00<00:00, 48.04batch/s, acc=1, loss=0.0149] \n",
      "Epoch 24: 100%|██████████| 41/41 [00:00<00:00, 61.67batch/s, acc=1, loss=0.0122] \n",
      "Epoch 25: 100%|██████████| 41/41 [00:00<00:00, 49.71batch/s, acc=1, loss=0.0102] \n",
      "Epoch 26: 100%|██████████| 41/41 [00:00<00:00, 44.42batch/s, acc=1, loss=0.0087]  \n",
      "Epoch 27: 100%|██████████| 41/41 [00:00<00:00, 79.72batch/s, acc=1, loss=0.00748] \n",
      "Epoch 28: 100%|██████████| 41/41 [00:00<00:00, 81.29batch/s, acc=1, loss=0.00647] \n",
      "Epoch 29: 100%|██████████| 41/41 [00:01<00:00, 36.12batch/s, acc=1, loss=0.00568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/politics\n",
      "               True Left  False Left  True Right  False Right\n",
      "data/politics        459         240         614          305\n",
      "\n",
      "Dataset: data/white_house (about white_house)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 27/27 [00:00<00:00, 52.66batch/s, acc=0.667, loss=0.68]\n",
      "Epoch 1: 100%|██████████| 27/27 [00:01<00:00, 14.07batch/s, acc=0.667, loss=0.681]\n",
      "Epoch 2: 100%|██████████| 27/27 [00:01<00:00, 19.32batch/s, acc=0.667, loss=0.681]\n",
      "Epoch 3: 100%|██████████| 27/27 [00:01<00:00, 23.45batch/s, acc=0.667, loss=0.682]\n",
      "Epoch 4: 100%|██████████| 27/27 [00:03<00:00,  7.53batch/s, acc=0.667, loss=0.682]\n",
      "Epoch 5: 100%|██████████| 27/27 [00:03<00:00,  8.39batch/s, acc=0.667, loss=0.683]\n",
      "Epoch 6: 100%|██████████| 27/27 [00:02<00:00, 11.26batch/s, acc=0.667, loss=0.683]\n",
      "Epoch 7: 100%|██████████| 27/27 [00:00<00:00, 58.04batch/s, acc=0.667, loss=0.683]\n",
      "Epoch 8: 100%|██████████| 27/27 [00:00<00:00, 63.98batch/s, acc=0.667, loss=0.682]\n",
      "Epoch 9: 100%|██████████| 27/27 [00:00<00:00, 31.52batch/s, acc=0.667, loss=0.681]\n",
      "Epoch 10: 100%|██████████| 27/27 [00:02<00:00,  9.61batch/s, acc=0.889, loss=0.679]\n",
      "Epoch 11: 100%|██████████| 27/27 [00:01<00:00, 22.52batch/s, acc=1, loss=0.674]  \n",
      "Epoch 12: 100%|██████████| 27/27 [00:01<00:00, 23.07batch/s, acc=1, loss=0.667]  \n",
      "Epoch 13: 100%|██████████| 27/27 [00:00<00:00, 33.62batch/s, acc=1, loss=0.656]  \n",
      "Epoch 14: 100%|██████████| 27/27 [00:01<00:00, 19.27batch/s, acc=1, loss=0.638]  \n",
      "Epoch 15: 100%|██████████| 27/27 [00:01<00:00, 21.01batch/s, acc=1, loss=0.614]  \n",
      "Epoch 16: 100%|██████████| 27/27 [00:00<00:00, 27.44batch/s, acc=1, loss=0.582]  \n",
      "Epoch 17: 100%|██████████| 27/27 [00:00<00:00, 61.65batch/s, acc=1, loss=0.543]  \n",
      "Epoch 18: 100%|██████████| 27/27 [00:00<00:00, 56.45batch/s, acc=1, loss=0.495]  \n",
      "Epoch 19: 100%|██████████| 27/27 [00:00<00:00, 60.59batch/s, acc=1, loss=0.44]   \n",
      "Epoch 20: 100%|██████████| 27/27 [00:00<00:00, 44.78batch/s, acc=1, loss=0.379]  \n",
      "Epoch 21: 100%|██████████| 27/27 [00:00<00:00, 54.09batch/s, acc=1, loss=0.317]  \n",
      "Epoch 22: 100%|██████████| 27/27 [00:00<00:00, 29.30batch/s, acc=1, loss=0.259]  \n",
      "Epoch 23: 100%|██████████| 27/27 [00:01<00:00, 19.16batch/s, acc=1, loss=0.206]  \n",
      "Epoch 24: 100%|██████████| 27/27 [00:00<00:00, 55.17batch/s, acc=1, loss=0.163]  \n",
      "Epoch 25: 100%|██████████| 27/27 [00:00<00:00, 53.45batch/s, acc=1, loss=0.128]  \n",
      "Epoch 26: 100%|██████████| 27/27 [00:02<00:00, 12.01batch/s, acc=1, loss=0.102]  \n",
      "Epoch 27: 100%|██████████| 27/27 [00:00<00:00, 30.07batch/s, acc=1, loss=0.0814] \n",
      "Epoch 28: 100%|██████████| 27/27 [00:01<00:00, 20.30batch/s, acc=1, loss=0.066]  \n",
      "Epoch 29: 100%|██████████| 27/27 [00:00<00:00, 58.29batch/s, acc=1, loss=0.054]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/white_house\n",
      "                  True Left  False Left  True Right  False Right\n",
      "data/white_house        334         205         364          179\n",
      "\n",
      "Dataset: data/immigration (about immigration)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 23/23 [00:00<00:00, 24.98batch/s, acc=1, loss=0.672]  \n",
      "Epoch 1: 100%|██████████| 23/23 [00:02<00:00, 10.15batch/s, acc=1, loss=0.666]  \n",
      "Epoch 2: 100%|██████████| 23/23 [00:00<00:00, 57.79batch/s, acc=1, loss=0.661]   \n",
      "Epoch 3: 100%|██████████| 23/23 [00:01<00:00, 19.87batch/s, acc=1, loss=0.654]  \n",
      "Epoch 4: 100%|██████████| 23/23 [00:00<00:00, 58.62batch/s, acc=1, loss=0.648]  \n",
      "Epoch 5: 100%|██████████| 23/23 [00:00<00:00, 27.90batch/s, acc=1, loss=0.641]  \n",
      "Epoch 6: 100%|██████████| 23/23 [00:01<00:00, 14.06batch/s, acc=1, loss=0.633]  \n",
      "Epoch 7: 100%|██████████| 23/23 [00:00<00:00, 37.77batch/s, acc=1, loss=0.625]  \n",
      "Epoch 8: 100%|██████████| 23/23 [00:00<00:00, 51.83batch/s, acc=1, loss=0.616]  \n",
      "Epoch 9: 100%|██████████| 23/23 [00:00<00:00, 52.05batch/s, acc=1, loss=0.606]  \n",
      "Epoch 10: 100%|██████████| 23/23 [00:01<00:00, 22.42batch/s, acc=1, loss=0.594]  \n",
      "Epoch 11: 100%|██████████| 23/23 [00:00<00:00, 51.80batch/s, acc=1, loss=0.581]  \n",
      "Epoch 12: 100%|██████████| 23/23 [00:00<00:00, 53.24batch/s, acc=1, loss=0.564]  \n",
      "Epoch 13: 100%|██████████| 23/23 [00:00<00:00, 56.85batch/s, acc=1, loss=0.543]  \n",
      "Epoch 14: 100%|██████████| 23/23 [00:00<00:00, 70.73batch/s, acc=1, loss=0.517]  \n",
      "Epoch 15: 100%|██████████| 23/23 [00:00<00:00, 49.20batch/s, acc=1, loss=0.487]  \n",
      "Epoch 16: 100%|██████████| 23/23 [00:01<00:00, 16.39batch/s, acc=1, loss=0.453]  \n",
      "Epoch 17: 100%|██████████| 23/23 [00:00<00:00, 46.89batch/s, acc=1, loss=0.413]  \n",
      "Epoch 18: 100%|██████████| 23/23 [00:00<00:00, 51.11batch/s, acc=1, loss=0.368]  \n",
      "Epoch 19: 100%|██████████| 23/23 [00:00<00:00, 27.82batch/s, acc=1, loss=0.318]  \n",
      "Epoch 20: 100%|██████████| 23/23 [00:01<00:00, 16.03batch/s, acc=1, loss=0.266]  \n",
      "Epoch 21: 100%|██████████| 23/23 [00:00<00:00, 28.31batch/s, acc=1, loss=0.214]  \n",
      "Epoch 22: 100%|██████████| 23/23 [00:00<00:00, 66.22batch/s, acc=1, loss=0.165]   \n",
      "Epoch 23: 100%|██████████| 23/23 [00:00<00:00, 50.48batch/s, acc=1, loss=0.12]   \n",
      "Epoch 24: 100%|██████████| 23/23 [00:01<00:00, 17.79batch/s, acc=1, loss=0.0849] \n",
      "Epoch 25: 100%|██████████| 23/23 [00:00<00:00, 42.92batch/s, acc=1, loss=0.0593] \n",
      "Epoch 26: 100%|██████████| 23/23 [00:00<00:00, 33.05batch/s, acc=1, loss=0.0406] \n",
      "Epoch 27: 100%|██████████| 23/23 [00:00<00:00, 40.73batch/s, acc=1, loss=0.0279] \n",
      "Epoch 28: 100%|██████████| 23/23 [00:00<00:00, 53.40batch/s, acc=1, loss=0.0195] \n",
      "Epoch 29: 100%|██████████| 23/23 [00:00<00:00, 45.23batch/s, acc=1, loss=0.0139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/immigration\n",
      "                  True Left  False Left  True Right  False Right\n",
      "data/immigration        184          67         457          207\n",
      "\n",
      "Dataset: data/healthcare (about healthcare)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 22/22 [00:00<00:00, 75.17batch/s, acc=0.667, loss=0.692] \n",
      "Epoch 1: 100%|██████████| 22/22 [00:00<00:00, 68.17batch/s, acc=0.333, loss=0.693]\n",
      "Epoch 2: 100%|██████████| 22/22 [00:00<00:00, 53.20batch/s, acc=0.333, loss=0.694]\n",
      "Epoch 3: 100%|██████████| 22/22 [00:00<00:00, 80.83batch/s, acc=0.333, loss=0.694] \n",
      "Epoch 4: 100%|██████████| 22/22 [00:00<00:00, 62.90batch/s, acc=0.333, loss=0.694]\n",
      "Epoch 5: 100%|██████████| 22/22 [00:00<00:00, 81.67batch/s, acc=0.333, loss=0.694]\n",
      "Epoch 6: 100%|██████████| 22/22 [00:00<00:00, 89.51batch/s, acc=0.333, loss=0.694]\n",
      "Epoch 7: 100%|██████████| 22/22 [00:00<00:00, 55.49batch/s, acc=0.333, loss=0.692]\n",
      "Epoch 8: 100%|██████████| 22/22 [00:00<00:00, 63.36batch/s, acc=0.333, loss=0.691]\n",
      "Epoch 9: 100%|██████████| 22/22 [00:00<00:00, 73.28batch/s, acc=0.333, loss=0.688]\n",
      "Epoch 10: 100%|██████████| 22/22 [00:00<00:00, 79.72batch/s, acc=0.333, loss=0.683]\n",
      "Epoch 11: 100%|██████████| 22/22 [00:00<00:00, 74.57batch/s, acc=0.333, loss=0.676]\n",
      "Epoch 12: 100%|██████████| 22/22 [00:00<00:00, 62.79batch/s, acc=0.333, loss=0.666]\n",
      "Epoch 13: 100%|██████████| 22/22 [00:00<00:00, 60.57batch/s, acc=0.667, loss=0.653]\n",
      "Epoch 14: 100%|██████████| 22/22 [00:00<00:00, 87.28batch/s, acc=1, loss=0.635]   \n",
      "Epoch 15: 100%|██████████| 22/22 [00:00<00:00, 75.23batch/s, acc=1, loss=0.611]   \n",
      "Epoch 16: 100%|██████████| 22/22 [00:00<00:00, 61.34batch/s, acc=1, loss=0.583]  \n",
      "Epoch 17: 100%|██████████| 22/22 [00:00<00:00, 77.40batch/s, acc=1, loss=0.55]    \n",
      "Epoch 18: 100%|██████████| 22/22 [00:00<00:00, 83.37batch/s, acc=1, loss=0.508]   \n",
      "Epoch 19: 100%|██████████| 22/22 [00:00<00:00, 53.75batch/s, acc=1, loss=0.459]  \n",
      "Epoch 20: 100%|██████████| 22/22 [00:00<00:00, 70.40batch/s, acc=1, loss=0.402]  \n",
      "Epoch 21: 100%|██████████| 22/22 [00:00<00:00, 79.98batch/s, acc=1, loss=0.342]  \n",
      "Epoch 22: 100%|██████████| 22/22 [00:00<00:00, 73.64batch/s, acc=1, loss=0.283]  \n",
      "Epoch 23: 100%|██████████| 22/22 [00:00<00:00, 88.03batch/s, acc=1, loss=0.227]   \n",
      "Epoch 24: 100%|██████████| 22/22 [00:00<00:00, 84.53batch/s, acc=1, loss=0.177]  \n",
      "Epoch 25: 100%|██████████| 22/22 [00:00<00:00, 89.70batch/s, acc=1, loss=0.136]   \n",
      "Epoch 26: 100%|██████████| 22/22 [00:00<00:00, 34.24batch/s, acc=1, loss=0.103]  \n",
      "Epoch 27: 100%|██████████| 22/22 [00:00<00:00, 73.81batch/s, acc=1, loss=0.0778]  \n",
      "Epoch 28: 100%|██████████| 22/22 [00:00<00:00, 81.56batch/s, acc=1, loss=0.059]  \n",
      "Epoch 29: 100%|██████████| 22/22 [00:00<00:00, 86.22batch/s, acc=1, loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/healthcare\n",
      "                 True Left  False Left  True Right  False Right\n",
      "data/healthcare        182          62         419          240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = pd.DataFrame(None, urls, ['F1 Macro', 'F1 Micro', 'Precision', 'Recall'])\n",
    "cms = pd.DataFrame(None, [], ['True Left', 'False Left', 'True Right', 'False Right'])\n",
    "\n",
    "saveplace = init_saveplace()\n",
    "\n",
    "for url in urls:\n",
    "    topic = url.split('/')[-1]\n",
    "    print(f'Dataset: {url} (about {topic})')\n",
    "    print(f'Processing dataset')\n",
    "    x_train, y_train, x_test, y_test, test_data = process_dataset(url, need_dense=False)\n",
    "\n",
    "    print(f'Training model')\n",
    "    model = train_and_save(x_train, y_train, x_test, y_test, saveplace, topic)\n",
    "    print(f'Testing model')\n",
    "    test_data = test_and_save(model, x_test, test_data, saveplace, topic)\n",
    "\n",
    "    cm = conf_mat(url, test_data)\n",
    "    print(f'Computing statistics')\n",
    "    print(f'{url}\\n{cm}\\n')\n",
    "    cms = pd.concat([cms, cm], axis=0)\n",
    "\n",
    "    true_bias, pred_bias = test_data['bias'], test_data['pred_bias']\n",
    "    scores['F1 Macro'][url] = f1_score(true_bias, pred_bias, average='macro')\n",
    "    scores['F1 Micro'][url] = f1_score(true_bias, pred_bias, average='micro')\n",
    "    scores['Precision'][url] = average_precision_score(true_bias, pred_bias)\n",
    "    scores['Recall'][url] = recall_score(true_bias, pred_bias)\n",
    "\n",
    "combined_info = pd.concat([scores, cms], axis=1)\n",
    "\n",
    "combined_info.to_csv(os.path.join(saveplace, 'nn_tsvd_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
