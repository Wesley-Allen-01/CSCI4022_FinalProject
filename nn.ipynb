{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stop_words as sw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import average_precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['data/elections', 'data/politics', 'data/white_house', 'data/immigration', 'data/healthcare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(url):\n",
    "    train_data = pd.read_csv(f'{url}/train/{url.split(\"/\")[1]}_train.csv')\n",
    "    train_data = train_data[(train_data['bias'] == 0) | (train_data['bias'] == 2)].replace({'bias': {2: 1}})\n",
    "    train_data['stop_content'] = train_data['content'].apply(lambda x: ' '.join(sw.stop_words(x)))\n",
    "    \n",
    "    test_data = pd.read_csv(f'{url}/test/{url.split(\"/\")[1]}_test.csv')\n",
    "    test_data = test_data[(test_data['bias'] == 0) | (test_data['bias'] == 2)].replace({'bias': {2: 1}})\n",
    "    test_data['stop_content'] = test_data['content'].apply(lambda x: ' '.join(sw.stop_words(x)))\n",
    "    \n",
    "    tfid = TfidfVectorizer()\n",
    "    \n",
    "    x_train = tfid.fit_transform(train_data['stop_content'])\n",
    "    y_train = train_data['bias'].values\n",
    "    \n",
    "    x_test = tfid.transform(test_data['stop_content'])\n",
    "    y_test = test_data['bias'].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepText(nn.Module):\n",
    "    def __init__(self, tfidf_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(tfidf_size, 200)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(200, 100)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(100, 50)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val, n_epochs=30, batch_size=10):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    batch_start = torch.arange(0, X_train.shape[0], batch_size)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(url):\n",
    "    def xy_torchly(x, y):\n",
    "        x_torch = torch.tensor(x.todense(), dtype=torch.float32)\n",
    "        y_torch = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "        return x_torch, y_torch\n",
    "    x_train, y_train, x_test, y_test, _, test_data = preprocess_data(url)\n",
    "    x_train, y_train = xy_torchly(x_train, y_train)\n",
    "    x_test, y_test = xy_torchly(x_test, y_test)\n",
    "    return x_train, y_train, x_test, y_test, test_data\n",
    "\n",
    "def init_saveplace():\n",
    "    current = datetime.now().strftime(\"%d-%m-%y%H:%M:%S\")\n",
    "    route = os.path.join('.', 'runs', f'nn-run-{current}')\n",
    "    os.makedirs(route, exist_ok=True)\n",
    "    return route\n",
    "\n",
    "def train_and_save(x_train, y_train, x_test, y_test, saveplace, topic):\n",
    "    # Generate model\n",
    "    vec_size = x_train.shape[1]\n",
    "    model = DeepText(vec_size)\n",
    "\n",
    "    _ = model_train(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(saveplace, f'model_{topic}.pt'))\n",
    "    return model\n",
    "\n",
    "def test_and_save(model, x_test, test_data, saveplace, topic):\n",
    "    test_data['prediction'] = np.array([model(x).detach().numpy() for x in x_test])\n",
    "    test_data['pred_bias'] = test_data['prediction'].round().astype(int)\n",
    "\n",
    "    test_data.to_csv(os.path.join(saveplace, f'test_data_{topic}.csv'), index=False)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(data):\n",
    "    confusion_matrix = pd.DataFrame(0, ['Predicted Left', 'Predicted Right'], ['Actual Left', 'Actual Right'])\n",
    "    for prediction, actual in [[0,0], [0,1], [1,0], [1,1]]:\n",
    "        confusion_matrix[f'Actual {\"Left\" if actual == 0 else \"Right\"}'][f'Predicted {\"Left\" if prediction == 0 else \"Right\"}'] = \\\n",
    "            data[(data['pred_bias'] == prediction) & (data['bias'] == actual)].count()['pred_bias']\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: data/elections (about elections)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 83/83 [00:04<00:00, 18.19batch/s, acc=0, loss=0.715]  \n",
      "Epoch 1: 100%|██████████| 83/83 [00:04<00:00, 18.34batch/s, acc=1, loss=0.666]  \n",
      "Epoch 2: 100%|██████████| 83/83 [00:04<00:00, 16.64batch/s, acc=1, loss=0.533]  \n",
      "Epoch 3: 100%|██████████| 83/83 [00:04<00:00, 18.67batch/s, acc=1, loss=0.27]   \n",
      "Epoch 4: 100%|██████████| 83/83 [00:04<00:00, 18.75batch/s, acc=1, loss=0.0771] \n",
      "Epoch 5: 100%|██████████| 83/83 [00:04<00:00, 16.91batch/s, acc=1, loss=0.0216]\n",
      "Epoch 6: 100%|██████████| 83/83 [00:04<00:00, 18.38batch/s, acc=1, loss=0.00771]\n",
      "Epoch 7: 100%|██████████| 83/83 [00:04<00:00, 17.99batch/s, acc=1, loss=0.00408]\n",
      "Epoch 8: 100%|██████████| 83/83 [00:04<00:00, 18.84batch/s, acc=1, loss=0.0026] \n",
      "Epoch 9: 100%|██████████| 83/83 [00:05<00:00, 16.39batch/s, acc=1, loss=0.00171]\n",
      "Epoch 10: 100%|██████████| 83/83 [00:04<00:00, 16.62batch/s, acc=1, loss=0.00119]\n",
      "Epoch 11: 100%|██████████| 83/83 [00:04<00:00, 17.61batch/s, acc=1, loss=0.000865]\n",
      "Epoch 12: 100%|██████████| 83/83 [00:05<00:00, 16.36batch/s, acc=1, loss=0.000659]\n",
      "Epoch 13: 100%|██████████| 83/83 [00:04<00:00, 18.02batch/s, acc=1, loss=0.000522]\n",
      "Epoch 14: 100%|██████████| 83/83 [00:04<00:00, 18.05batch/s, acc=1, loss=0.000424]\n",
      "Epoch 15: 100%|██████████| 83/83 [00:05<00:00, 15.47batch/s, acc=1, loss=0.000349]\n",
      "Epoch 16: 100%|██████████| 83/83 [00:04<00:00, 18.40batch/s, acc=1, loss=0.000292]\n",
      "Epoch 17: 100%|██████████| 83/83 [00:04<00:00, 17.95batch/s, acc=1, loss=0.000249]\n",
      "Epoch 18: 100%|██████████| 83/83 [00:04<00:00, 17.94batch/s, acc=1, loss=0.000214]\n",
      "Epoch 19: 100%|██████████| 83/83 [00:04<00:00, 17.44batch/s, acc=1, loss=0.000186]\n",
      "Epoch 20: 100%|██████████| 83/83 [00:05<00:00, 16.22batch/s, acc=1, loss=0.000164]\n",
      "Epoch 21: 100%|██████████| 83/83 [00:04<00:00, 17.46batch/s, acc=1, loss=0.000145]\n",
      "Epoch 22: 100%|██████████| 83/83 [00:05<00:00, 16.26batch/s, acc=1, loss=0.000129]\n",
      "Epoch 23: 100%|██████████| 83/83 [00:04<00:00, 17.59batch/s, acc=1, loss=0.000115]\n",
      "Epoch 24: 100%|██████████| 83/83 [00:05<00:00, 16.03batch/s, acc=1, loss=9.74e-5] \n",
      "Epoch 25: 100%|██████████| 83/83 [00:04<00:00, 17.70batch/s, acc=1, loss=8.07e-5] \n",
      "Epoch 26: 100%|██████████| 83/83 [00:04<00:00, 17.32batch/s, acc=1, loss=6.67e-5] \n",
      "Epoch 27: 100%|██████████| 83/83 [00:05<00:00, 16.13batch/s, acc=1, loss=5.53e-5] \n",
      "Epoch 28: 100%|██████████| 83/83 [00:04<00:00, 17.41batch/s, acc=1, loss=4.62e-5] \n",
      "Epoch 29: 100%|██████████| 83/83 [00:05<00:00, 15.63batch/s, acc=1, loss=3.89e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/elections\n",
      "                 Actual Left  Actual Right\n",
      "Predicted Left          1651           739\n",
      "Predicted Right          284           737\n",
      "\n",
      "Dataset: data/politics (about politics)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 41/41 [00:01<00:00, 20.66batch/s, acc=0.667, loss=0.693]\n",
      "Epoch 1: 100%|██████████| 41/41 [00:02<00:00, 19.20batch/s, acc=0.667, loss=0.689]\n",
      "Epoch 2: 100%|██████████| 41/41 [00:01<00:00, 21.60batch/s, acc=1, loss=0.676]  \n",
      "Epoch 3: 100%|██████████| 41/41 [00:03<00:00, 13.12batch/s, acc=1, loss=0.642]  \n",
      "Epoch 4: 100%|██████████| 41/41 [00:03<00:00, 12.03batch/s, acc=1, loss=0.571]  \n",
      "Epoch 5: 100%|██████████| 41/41 [00:03<00:00, 11.82batch/s, acc=1, loss=0.445]  \n",
      "Epoch 6: 100%|██████████| 41/41 [00:05<00:00,  7.90batch/s, acc=1, loss=0.278]  \n",
      "Epoch 7: 100%|██████████| 41/41 [00:05<00:00,  7.66batch/s, acc=1, loss=0.145]\n",
      "Epoch 8: 100%|██████████| 41/41 [00:03<00:00, 11.44batch/s, acc=1, loss=0.071] \n",
      "Epoch 9: 100%|██████████| 41/41 [00:05<00:00,  7.71batch/s, acc=1, loss=0.0383]\n",
      "Epoch 10: 100%|██████████| 41/41 [00:05<00:00,  7.52batch/s, acc=1, loss=0.0231]\n",
      "Epoch 11: 100%|██████████| 41/41 [00:03<00:00, 11.85batch/s, acc=1, loss=0.0154]\n",
      "Epoch 12: 100%|██████████| 41/41 [00:03<00:00, 10.61batch/s, acc=1, loss=0.0109] \n",
      "Epoch 13: 100%|██████████| 41/41 [00:03<00:00, 10.91batch/s, acc=1, loss=0.00804]\n",
      "Epoch 14: 100%|██████████| 41/41 [00:01<00:00, 21.18batch/s, acc=1, loss=0.0062] \n",
      "Epoch 15: 100%|██████████| 41/41 [00:02<00:00, 18.15batch/s, acc=1, loss=0.00493]\n",
      "Epoch 16: 100%|██████████| 41/41 [00:01<00:00, 20.75batch/s, acc=1, loss=0.004]  \n",
      "Epoch 17: 100%|██████████| 41/41 [00:03<00:00, 11.20batch/s, acc=1, loss=0.0033] \n",
      "Epoch 18: 100%|██████████| 41/41 [00:02<00:00, 15.31batch/s, acc=1, loss=0.00277]\n",
      "Epoch 19: 100%|██████████| 41/41 [00:02<00:00, 18.12batch/s, acc=1, loss=0.00235]\n",
      "Epoch 20: 100%|██████████| 41/41 [00:01<00:00, 22.17batch/s, acc=1, loss=0.00202]\n",
      "Epoch 21: 100%|██████████| 41/41 [00:03<00:00, 10.98batch/s, acc=1, loss=0.00175]\n",
      "Epoch 22: 100%|██████████| 41/41 [00:02<00:00, 14.13batch/s, acc=1, loss=0.00153] \n",
      "Epoch 23: 100%|██████████| 41/41 [00:02<00:00, 15.97batch/s, acc=1, loss=0.00135] \n",
      "Epoch 24: 100%|██████████| 41/41 [00:02<00:00, 18.10batch/s, acc=1, loss=0.0012]  \n",
      "Epoch 25: 100%|██████████| 41/41 [00:03<00:00, 12.46batch/s, acc=1, loss=0.00107] \n",
      "Epoch 26: 100%|██████████| 41/41 [00:03<00:00, 12.48batch/s, acc=1, loss=0.000957]\n",
      "Epoch 27: 100%|██████████| 41/41 [00:02<00:00, 14.14batch/s, acc=1, loss=0.000861]\n",
      "Epoch 28: 100%|██████████| 41/41 [00:04<00:00,  9.76batch/s, acc=1, loss=0.000779]\n",
      "Epoch 29: 100%|██████████| 41/41 [00:02<00:00, 19.32batch/s, acc=1, loss=0.000707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/politics\n",
      "                 Actual Left  Actual Right\n",
      "Predicted Left           462           210\n",
      "Predicted Right          302           644\n",
      "\n",
      "Dataset: data/white_house (about white_house)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 27/27 [00:01<00:00, 22.35batch/s, acc=0.667, loss=0.691]\n",
      "Epoch 1: 100%|██████████| 27/27 [00:01<00:00, 19.97batch/s, acc=0.667, loss=0.69]\n",
      "Epoch 2: 100%|██████████| 27/27 [00:01<00:00, 23.39batch/s, acc=0.667, loss=0.687]\n",
      "Epoch 3: 100%|██████████| 27/27 [00:01<00:00, 19.89batch/s, acc=1, loss=0.681]  \n",
      "Epoch 4: 100%|██████████| 27/27 [00:01<00:00, 22.24batch/s, acc=1, loss=0.668] \n",
      "Epoch 5: 100%|██████████| 27/27 [00:01<00:00, 20.88batch/s, acc=1, loss=0.644] \n",
      "Epoch 6: 100%|██████████| 27/27 [00:01<00:00, 21.04batch/s, acc=1, loss=0.603] \n",
      "Epoch 7: 100%|██████████| 27/27 [00:01<00:00, 18.65batch/s, acc=1, loss=0.54]  \n",
      "Epoch 8: 100%|██████████| 27/27 [00:02<00:00, 12.73batch/s, acc=1, loss=0.45] \n",
      "Epoch 9: 100%|██████████| 27/27 [00:01<00:00, 15.26batch/s, acc=1, loss=0.344]\n",
      "Epoch 10: 100%|██████████| 27/27 [00:01<00:00, 18.89batch/s, acc=1, loss=0.243]\n",
      "Epoch 11: 100%|██████████| 27/27 [00:01<00:00, 19.50batch/s, acc=1, loss=0.167]\n",
      "Epoch 12: 100%|██████████| 27/27 [00:01<00:00, 17.94batch/s, acc=1, loss=0.108] \n",
      "Epoch 13: 100%|██████████| 27/27 [00:01<00:00, 17.27batch/s, acc=1, loss=0.07]  \n",
      "Epoch 14: 100%|██████████| 27/27 [00:01<00:00, 16.59batch/s, acc=1, loss=0.049] \n",
      "Epoch 15: 100%|██████████| 27/27 [00:01<00:00, 19.43batch/s, acc=1, loss=0.0358]\n",
      "Epoch 16: 100%|██████████| 27/27 [00:01<00:00, 17.87batch/s, acc=1, loss=0.0267]\n",
      "Epoch 17: 100%|██████████| 27/27 [00:01<00:00, 17.55batch/s, acc=1, loss=0.0207]\n",
      "Epoch 18: 100%|██████████| 27/27 [00:01<00:00, 19.99batch/s, acc=1, loss=0.0165]\n",
      "Epoch 19: 100%|██████████| 27/27 [00:01<00:00, 17.51batch/s, acc=1, loss=0.0134] \n",
      "Epoch 20: 100%|██████████| 27/27 [00:02<00:00, 13.34batch/s, acc=1, loss=0.0111] \n",
      "Epoch 21: 100%|██████████| 27/27 [00:01<00:00, 19.29batch/s, acc=1, loss=0.00932]\n",
      "Epoch 22: 100%|██████████| 27/27 [00:01<00:00, 17.06batch/s, acc=1, loss=0.00791]\n",
      "Epoch 23: 100%|██████████| 27/27 [00:01<00:00, 19.95batch/s, acc=1, loss=0.0068] \n",
      "Epoch 24: 100%|██████████| 27/27 [00:01<00:00, 23.15batch/s, acc=1, loss=0.0059] \n",
      "Epoch 25: 100%|██████████| 27/27 [00:01<00:00, 18.73batch/s, acc=1, loss=0.00516]\n",
      "Epoch 26: 100%|██████████| 27/27 [00:01<00:00, 18.21batch/s, acc=1, loss=0.00455]\n",
      "Epoch 27: 100%|██████████| 27/27 [00:01<00:00, 24.98batch/s, acc=1, loss=0.00404]\n",
      "Epoch 28: 100%|██████████| 27/27 [00:02<00:00, 12.83batch/s, acc=1, loss=0.00362]\n",
      "Epoch 29: 100%|██████████| 27/27 [00:03<00:00,  8.23batch/s, acc=1, loss=0.00325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/white_house\n",
      "                 Actual Left  Actual Right\n",
      "Predicted Left           349           225\n",
      "Predicted Right          164           344\n",
      "\n",
      "Dataset: data/immigration (about immigration)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 23/23 [00:01<00:00, 16.17batch/s, acc=0, loss=0.756]  \n",
      "Epoch 1: 100%|██████████| 23/23 [00:01<00:00, 22.51batch/s, acc=0, loss=0.747]  \n",
      "Epoch 2: 100%|██████████| 23/23 [00:01<00:00, 22.06batch/s, acc=0, loss=0.734]  \n",
      "Epoch 3: 100%|██████████| 23/23 [00:01<00:00, 17.38batch/s, acc=0, loss=0.718]  \n",
      "Epoch 4: 100%|██████████| 23/23 [00:00<00:00, 26.15batch/s, acc=0, loss=0.699]  \n",
      "Epoch 5: 100%|██████████| 23/23 [00:01<00:00, 18.71batch/s, acc=1, loss=0.676]  \n",
      "Epoch 6: 100%|██████████| 23/23 [00:00<00:00, 25.98batch/s, acc=1, loss=0.642]  \n",
      "Epoch 7: 100%|██████████| 23/23 [00:01<00:00, 17.43batch/s, acc=1, loss=0.585]\n",
      "Epoch 8: 100%|██████████| 23/23 [00:01<00:00, 16.91batch/s, acc=1, loss=0.503]\n",
      "Epoch 9: 100%|██████████| 23/23 [00:01<00:00, 18.39batch/s, acc=1, loss=0.4]  \n",
      "Epoch 10: 100%|██████████| 23/23 [00:02<00:00, 11.33batch/s, acc=1, loss=0.292]\n",
      "Epoch 11: 100%|██████████| 23/23 [00:01<00:00, 13.27batch/s, acc=1, loss=0.195]\n",
      "Epoch 12: 100%|██████████| 23/23 [00:01<00:00, 21.40batch/s, acc=1, loss=0.126]\n",
      "Epoch 13: 100%|██████████| 23/23 [00:01<00:00, 19.13batch/s, acc=1, loss=0.0828]\n",
      "Epoch 14: 100%|██████████| 23/23 [00:01<00:00, 14.23batch/s, acc=1, loss=0.0559]\n",
      "Epoch 15: 100%|██████████| 23/23 [00:01<00:00, 17.69batch/s, acc=1, loss=0.0389]\n",
      "Epoch 16: 100%|██████████| 23/23 [00:01<00:00, 19.73batch/s, acc=1, loss=0.0286]\n",
      "Epoch 17: 100%|██████████| 23/23 [00:01<00:00, 13.70batch/s, acc=1, loss=0.0216]\n",
      "Epoch 18: 100%|██████████| 23/23 [00:01<00:00, 14.02batch/s, acc=1, loss=0.0168]\n",
      "Epoch 19: 100%|██████████| 23/23 [00:01<00:00, 20.99batch/s, acc=1, loss=0.0134]\n",
      "Epoch 20: 100%|██████████| 23/23 [00:01<00:00, 21.71batch/s, acc=1, loss=0.0109]\n",
      "Epoch 21: 100%|██████████| 23/23 [00:01<00:00, 16.89batch/s, acc=1, loss=0.00901]\n",
      "Epoch 22: 100%|██████████| 23/23 [00:02<00:00,  9.32batch/s, acc=1, loss=0.00755]\n",
      "Epoch 23: 100%|██████████| 23/23 [00:02<00:00, 10.20batch/s, acc=1, loss=0.0064] \n",
      "Epoch 24: 100%|██████████| 23/23 [00:01<00:00, 14.01batch/s, acc=1, loss=0.0055] \n",
      "Epoch 25: 100%|██████████| 23/23 [00:01<00:00, 17.09batch/s, acc=1, loss=0.00477]\n",
      "Epoch 26: 100%|██████████| 23/23 [00:00<00:00, 24.12batch/s, acc=1, loss=0.00418]\n",
      "Epoch 27: 100%|██████████| 23/23 [00:01<00:00, 20.68batch/s, acc=1, loss=0.00368]\n",
      "Epoch 28: 100%|██████████| 23/23 [00:01<00:00, 22.17batch/s, acc=1, loss=0.00328]\n",
      "Epoch 29: 100%|██████████| 23/23 [00:01<00:00, 14.12batch/s, acc=1, loss=0.00293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/immigration\n",
      "                 Actual Left  Actual Right\n",
      "Predicted Left           196            71\n",
      "Predicted Right          195           453\n",
      "\n",
      "Dataset: data/healthcare (about healthcare)\n",
      "Processing dataset\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 22/22 [00:01<00:00, 16.08batch/s, acc=0.667, loss=0.679]\n",
      "Epoch 1: 100%|██████████| 22/22 [00:01<00:00, 17.83batch/s, acc=0.667, loss=0.679]\n",
      "Epoch 2: 100%|██████████| 22/22 [00:01<00:00, 18.57batch/s, acc=0.667, loss=0.677]\n",
      "Epoch 3: 100%|██████████| 22/22 [00:01<00:00, 20.40batch/s, acc=0.667, loss=0.67]\n",
      "Epoch 4: 100%|██████████| 22/22 [00:01<00:00, 15.47batch/s, acc=0.667, loss=0.656]\n",
      "Epoch 5: 100%|██████████| 22/22 [00:01<00:00, 11.92batch/s, acc=1, loss=0.634]  \n",
      "Epoch 6: 100%|██████████| 22/22 [00:00<00:00, 27.35batch/s, acc=1, loss=0.601] \n",
      "Epoch 7: 100%|██████████| 22/22 [00:01<00:00, 19.51batch/s, acc=1, loss=0.556]\n",
      "Epoch 8: 100%|██████████| 22/22 [00:01<00:00, 14.36batch/s, acc=1, loss=0.493]\n",
      "Epoch 9: 100%|██████████| 22/22 [00:00<00:00, 26.06batch/s, acc=1, loss=0.408]\n",
      "Epoch 10: 100%|██████████| 22/22 [00:01<00:00, 19.89batch/s, acc=1, loss=0.307]\n",
      "Epoch 11: 100%|██████████| 22/22 [00:01<00:00, 21.30batch/s, acc=1, loss=0.209]\n",
      "Epoch 12: 100%|██████████| 22/22 [00:01<00:00, 18.18batch/s, acc=1, loss=0.134]\n",
      "Epoch 13: 100%|██████████| 22/22 [00:01<00:00, 19.05batch/s, acc=1, loss=0.0878]\n",
      "Epoch 14: 100%|██████████| 22/22 [00:01<00:00, 12.90batch/s, acc=1, loss=0.0568]\n",
      "Epoch 15: 100%|██████████| 22/22 [00:00<00:00, 24.33batch/s, acc=1, loss=0.0381]\n",
      "Epoch 16: 100%|██████████| 22/22 [00:01<00:00, 20.20batch/s, acc=1, loss=0.027] \n",
      "Epoch 17: 100%|██████████| 22/22 [00:01<00:00, 12.33batch/s, acc=1, loss=0.0199]\n",
      "Epoch 18: 100%|██████████| 22/22 [00:00<00:00, 27.09batch/s, acc=1, loss=0.0151]\n",
      "Epoch 19: 100%|██████████| 22/22 [00:00<00:00, 23.04batch/s, acc=1, loss=0.0118]\n",
      "Epoch 20: 100%|██████████| 22/22 [00:01<00:00, 19.19batch/s, acc=1, loss=0.00915]\n",
      "Epoch 21: 100%|██████████| 22/22 [00:01<00:00, 12.61batch/s, acc=1, loss=0.00725]\n",
      "Epoch 22: 100%|██████████| 22/22 [00:01<00:00, 21.93batch/s, acc=1, loss=0.00587]\n",
      "Epoch 23: 100%|██████████| 22/22 [00:01<00:00, 13.47batch/s, acc=1, loss=0.00485]\n",
      "Epoch 24: 100%|██████████| 22/22 [00:01<00:00, 15.25batch/s, acc=1, loss=0.00407]\n",
      "Epoch 25: 100%|██████████| 22/22 [00:01<00:00, 19.91batch/s, acc=1, loss=0.00346]\n",
      "Epoch 26: 100%|██████████| 22/22 [00:00<00:00, 23.19batch/s, acc=1, loss=0.00298]\n",
      "Epoch 27: 100%|██████████| 22/22 [00:01<00:00, 17.53batch/s, acc=1, loss=0.00259]\n",
      "Epoch 28: 100%|██████████| 22/22 [00:00<00:00, 24.00batch/s, acc=1, loss=0.00226]\n",
      "Epoch 29: 100%|██████████| 22/22 [00:01<00:00, 21.90batch/s, acc=1, loss=0.002]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Computing statistics\n",
      "data/healthcare\n",
      "                 Actual Left  Actual Right\n",
      "Predicted Left           285           139\n",
      "Predicted Right          137           342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = pd.DataFrame(None, ['f1_macro', 'f1_micro', 'precision', 'recall'], urls)\n",
    "\n",
    "saveplace = init_saveplace()\n",
    "\n",
    "for url in urls:\n",
    "    topic = url.split('/')[-1]\n",
    "    print(f'Dataset: {url} (about {topic})')\n",
    "    print(f'Processing dataset')\n",
    "    x_train, y_train, x_test, y_test, test_data = process_dataset(url)\n",
    "\n",
    "    print(f'Training model')\n",
    "    model = train_and_save(x_train, y_train, x_test, y_test, saveplace, topic)\n",
    "    print(f'Testing model')\n",
    "    test_data = test_and_save(model, x_test, test_data, saveplace, topic)\n",
    "\n",
    "    print(f'Computing statistics')\n",
    "    print(f'{url}\\n{conf_mat(test_data)}\\n')\n",
    "\n",
    "    true_bias, pred_bias = test_data['bias'], test_data['pred_bias']\n",
    "    scores[url]['f1_macro'] = f1_score(true_bias, pred_bias, average='macro')\n",
    "    scores[url]['f1_micro'] = f1_score(true_bias, pred_bias, average='micro')\n",
    "    scores[url]['precision'] = average_precision_score(true_bias, pred_bias)\n",
    "    scores[url]['recall'] = recall_score(true_bias, pred_bias)\n",
    "\n",
    "scores.to_csv(os.path.join(saveplace, 'scores.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
